# Framer Documentation AI Assistant

Интеллектуальный ассистент для работы с документацией Framer, построенный на основе современных технологий обработки естественного языка и векторных баз данных. Система обеспечивает эффективный поиск и извлечение информации из документации с помощью продвинутой RAG (Retrieval-Augmented Generation) архитектуры.

## Особенности

- **Параллельный парсинг документации**
  - Асинхронный краулинг с контролем параллелизма
  - Оптимизированная обработка больших объемов данных
  - Кэширование результатов для повышения производительности

- **Продвинутый RAG с агентным подходом**
  - Семантический поиск с использованием векторных эмбеддингов
  - Динамическое определение релевантности контента
  - Автономное принятие решений агентом

- **Универсальное хранилище на Supabase**
  - Векторное хранение эмбеддингов
  - Эффективные индексы для быстрого поиска
  - Масштабируемая архитектура

- **Интерактивный интерфейс**
  - Streamlit для удобного взаимодействия
  - Потоковая передача ответов
  - Поддержка сложных запросов

## Требования

- Python 3.11+
- Supabase аккаунт и база данных
- OpenAI API ключ
- Streamlit (для веб-интерфейса)

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/your-username/crawl4AI-agent.git
cd crawl4AI-agent
```

2. Установите зависимости (рекомендуется использовать виртуальное окружение Python):
```bash
python -m venv venv
source venv/bin/activate  # На Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. Настройте переменные окружения:
   - Переименуйте `.env.example` в `.env`
   - Отредактируйте `.env` с вашими API ключами и настройками:
   ```env
   OPENAI_API_KEY=your_openai_api_key
   SUPABASE_URL=your_supabase_url
   SUPABASE_SERVICE_KEY=your_supabase_service_key
   LLM_MODEL=gpt-4o-mini  # или ваша предпочитаемая модель OpenAI
   ```

## Использование

### Настройка базы данных

Выполните SQL команды из `site_pages.sql` для:
1. Создания необходимых таблиц
2. Включения векторного поиска
3. Настройки политик безопасности на уровне строк

В Supabase это можно сделать через вкладку "SQL Editor", вставив SQL код и нажав "Run".

### Парсинг документации

Для парсинга и сохранения документации в векторной базе данных:

```bash
python crawl_framer_docs.py
```

Это выполнит:
1. Получение URL из sitemap документации
2. Парсинг каждой страницы и разбиение на чанки
3. Генерацию эмбеддингов и сохранение в Supabase

### Веб-интерфейс Streamlit

Для интерактивного веб-интерфейса:

```bash
streamlit run framer_ui.py
```

Интерфейс будет доступен по адресу `http://localhost:8501`

## Конфигурация

### Схема базы данных

База данных Supabase использует следующую схему:
```sql
CREATE TABLE site_pages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    url TEXT,
    chunk_number INTEGER,
    title TEXT,
    summary TEXT,
    content TEXT,
    metadata JSONB,
    embedding VECTOR(1536)
);
```

### Конфигурация чанкинга

Чанкинг текста настраивается через параметры в `crawl_framer_docs.py`:
- `MAX_CHUNK_SIZE`: максимальный размер чанка (по умолчанию 1000 токенов)
- `MIN_CHUNK_SIZE`: минимальный размер чанка (по умолчанию 200 токенов)
- `OVERLAP_SIZE`: размер перекрытия между чанками (по умолчанию 100 токенов)

## Будущие улучшения

1. **Тестирование и оптимизация**
   - Комплексное тестирование системы
   - Оптимизация структуры и запросов базы данных
   - Улучшение производительности векторного поиска

2. **Контейниризация и развертывание**
   - Создание Docker-контейнеров для агента
   - Настройка CI/CD пайплайнов
   - Автоматизация развертывания

3. **Специализированные агенты**
   - Разработка группы агентов с узкой специализацией
   - Создание экспертов по различным видам документации
   - Оптимизация взаимодействия между агентами

4. **Мульти-агентная система**
   - Интеграция с OpenAI Agents SDK
   - Создание системы оркестрации агентов
   - Реализация сложных рабочих процессов